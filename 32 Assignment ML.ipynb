{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Q1: Overfitting and Underfitting\n",
    "overfitting_underfitting = \"\"\"### Q1: Overfitting and Underfitting\n",
    "**Overfitting** occurs when a machine learning model learns not only the underlying pattern in the training data but also noise and random fluctuations. This results in high accuracy on the training set but poor generalization to unseen data.\n",
    "\n",
    "**Underfitting** happens when a model is too simple to capture the underlying pattern in the data, leading to poor performance on both training and test datasets.\n",
    "\n",
    "**Consequences:**\n",
    "- Overfitting: Poor generalization, high variance, unreliable predictions.\n",
    "- Underfitting: High bias, poor accuracy, inability to learn from data.\n",
    "\n",
    "**Mitigation:**\n",
    "- Overfitting: Use regularization, increase training data, apply dropout, use simpler models.\n",
    "- Underfitting: Increase model complexity, reduce regularization, use more relevant features.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(overfitting_underfitting))\n",
    "\n",
    "# Q2: Reducing Overfitting\n",
    "reducing_overfitting = \"\"\"### Q2: Reducing Overfitting\n",
    "Some techniques to reduce overfitting include:\n",
    "1. **Regularization** (L1, L2) - Adds penalty to model complexity.\n",
    "2. **More Training Data** - Helps the model generalize better.\n",
    "3. **Early Stopping** - Stops training when validation loss starts increasing.\n",
    "4. **Dropout** - Randomly drops neurons in deep networks to prevent reliance on specific neurons.\n",
    "5. **Cross-Validation** - Ensures better generalization by training on multiple subsets.\n",
    "6. **Simpler Model** - Reducing complexity prevents memorization of noise.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(reducing_overfitting))\n",
    "\n",
    "# Q3: Underfitting and Scenarios\n",
    "underfitting_scenarios = \"\"\"### Q3: Underfitting and Scenarios\n",
    "**Underfitting** occurs when the model is too simple to learn patterns in data.\n",
    "\n",
    "**Scenarios where underfitting occurs:**\n",
    "1. Using a linear model for non-linear data.\n",
    "2. Using insufficient features.\n",
    "3. Too much regularization applied.\n",
    "4. Insufficient training time leading to incomplete learning.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(underfitting_scenarios))\n",
    "\n",
    "# Q4: Bias-Variance Tradeoff\n",
    "bias_variance_tradeoff = \"\"\"### Q4: Bias-Variance Tradeoff\n",
    "- **Bias** refers to errors due to overly simplistic assumptions. High bias leads to underfitting.\n",
    "- **Variance** refers to errors due to sensitivity to fluctuations in training data. High variance leads to overfitting.\n",
    "\n",
    "**Tradeoff:**\n",
    "- Low bias, high variance → Overfitting\n",
    "- High bias, low variance → Underfitting\n",
    "- A balance is needed to optimize generalization.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(bias_variance_tradeoff))\n",
    "\n",
    "# Q5: Detecting Overfitting and Underfitting\n",
    "detecting_fitting = \"\"\"### Q5: Detecting Overfitting and Underfitting\n",
    "- **Overfitting Detection:**\n",
    "  - Training accuracy high, but test accuracy low.\n",
    "  - High variance in cross-validation.\n",
    "\n",
    "- **Underfitting Detection:**\n",
    "  - Both training and test accuracy are low.\n",
    "  - High bias in predictions.\n",
    "\n",
    "**Solutions:**\n",
    "- Learning curves to visualize performance.\n",
    "- Cross-validation scores for validation.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(detecting_fitting))\n",
    "\n",
    "# Q6: Bias vs Variance\n",
    "bias_vs_variance = \"\"\"### Q6: Bias vs Variance\n",
    "| Aspect | High Bias (Underfitting) | High Variance (Overfitting) |\n",
    "|--------|-------------------------|----------------------------|\n",
    "| Error Type | Systematic error | Sensitivity to fluctuations |\n",
    "| Model Complexity | Low | High |\n",
    "| Performance | Poor on training & test data | Good on training, poor on test |\n",
    "| Example | Linear regression on complex data | Deep neural network with no regularization |\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(bias_vs_variance))\n",
    "\n",
    "# Q7: Regularization\n",
    "regularization = \"\"\"### Q7: Regularization\n",
    "Regularization is used to prevent overfitting by adding a penalty to the model complexity.\n",
    "\n",
    "**Common Techniques:**\n",
    "1. **L1 Regularization (Lasso):** Adds absolute weights penalty (\\(|w|\\)), leading to feature selection.\n",
    "2. **L2 Regularization (Ridge):** Adds squared weights penalty (\\(w^2\\)), reducing weight magnitudes.\n",
    "3. **Dropout:** Randomly deactivates neurons during training.\n",
    "4. **Early Stopping:** Stops training when validation error increases.\n",
    "\"\"\"\n",
    "cells.append(nbf.v4.new_markdown_cell(regularization))\n",
    "\n",
    "notebook['cells'] = cells\n",
    "\n",
    "with open(\"ml_overfitting_underfitting.ipynb\", \"w\") as f:\n",
    "    nbf.write(notebook, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
